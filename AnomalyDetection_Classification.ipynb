{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import collections\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "# import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\".\\data\\creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for any null values\n",
    "data.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('Class', data=data, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
    "\n",
    "bins = 50\n",
    "\n",
    "ax1.hist(df.Time[df.Class == 1], bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(df.Time[df.Class == 0], bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cyclical encoding of the seconds attribute\n",
    "seconds_in_day = 24*60*60\n",
    "\n",
    "df['sin_time'] = np.sin(2*np.pi*df.Time/seconds_in_day)\n",
    "df['cos_time'] = np.cos(2*np.pi*df.Time/seconds_in_day)\n",
    "\n",
    "df.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we model the data as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is using LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# Predict on testing set\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, lr_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, lr_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, lr_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, lr_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, lr_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "\n",
    "# f1 score : 0.7443946188340808\n",
    "# precision : 0.9120879120879121\n",
    "# recall : 0.6287878787878788\n",
    "# Average Precision-Recall Score : 0.5741980064260337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is using RandomForestClassifier\n",
    "\n",
    "# train model\n",
    "rfc = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, rfc_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, rfc_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, rfc_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, rfc_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, rfc_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "\n",
    "# f1 score : 0.8702928870292888\n",
    "# precision : 0.9719626168224299\n",
    "# recall : 0.7878787878787878\n",
    "# Average Precision-Recall Score : 0.7661819757862424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is using KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "knc_pred = neigh.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, knc_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, knc_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, knc_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, knc_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, knc_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is using SVC\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "SVM_pred = clf.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, SVM_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, SVM_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, SVM_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, SVM_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, SVM_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is using MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(activation = \"logistic\")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, mlp_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, mlp_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, mlp_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, mlp_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, mlp_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try these same classifiers with over sampling the under represented(fraud) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "not_fraud = X[X.Class==0]\n",
    "fraud = X[X.Class==1]\n",
    "\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "upsampled.Class.value_counts()\n",
    "\n",
    "y_train = upsampled.Class\n",
    "X_train = upsampled.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data using LogisticRegression\n",
    "\n",
    "upsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, upsampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, upsampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, upsampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, upsampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, upsampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "# f1 score : 0.12520237452779276\n",
    "# precision : 0.0674026728646136\n",
    "# recall : 0.8787878787878788\n",
    "# Average Precision-Recall Score : 0.059457364700293704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data using RandomForestClassifier\n",
    "\n",
    "upsampled = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, upsampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, upsampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, upsampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, upsampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, upsampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "\n",
    "# f1 score : 0.8713692946058091\n",
    "# precision : 0.963302752293578\n",
    "# recall : 0.7954545454545454\n",
    "# Average Precision-Recall Score : 0.7666427557921818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using KNeighborsClassifier\n",
    "\n",
    "upsampled = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, upsampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, upsampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, upsampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, upsampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, upsampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using SVC\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "upsampled = svm.SVC().fit(X_train, y_train)\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, upsampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, upsampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, upsampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, upsampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, upsampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using MLPClassifier\n",
    "\n",
    "upsampled = MLPClassifier(activation = \"logistic\").fit(X_train, y_train)\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, upsampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, upsampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, upsampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, upsampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, upsampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try under sampling the over-represented negative(non-fraud) class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Class.value_counts()\n",
    "\n",
    "y_train = downsampled.Class\n",
    "X_train = downsampled.drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the undersampled data using LogisticRegression\n",
    "\n",
    "undersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, undersampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, undersampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, undersampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, undersampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, undersampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "\n",
    "# f1 score : 0.0819614711033275\n",
    "# precision : 0.04296731546088873\n",
    "# recall : 0.8863636363636364\n",
    "# Average Precision-Recall Score : 0.03829533421635301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the undersampled data using RandomForestClassifier\n",
    "\n",
    "undersampled = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, undersampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, undersampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, undersampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, undersampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, undersampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)\n",
    "\n",
    "# f1 score : 0.14348097317529632\n",
    "# precision : 0.07817811012916383\n",
    "# recall : 0.8712121212121212\n",
    "# Average Precision-Recall Score : 0.06834847449626065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using KNeighborsClassifier\n",
    "\n",
    "undersampled = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, undersampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, undersampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, undersampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, undersampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, undersampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using SVC\n",
    "\n",
    "undersampled = svm.SVC().fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, undersampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, undersampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, undersampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, undersampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, undersampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using MLPClassifier\n",
    "\n",
    "undersampled = MLPClassifier(activation = \"logistic\").fit(X_train, y_train)\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, undersampled_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, undersampled_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, undersampled_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, undersampled_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, undersampled_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try over-sampling the under-represented class using SMOTE(Synthetic Minority Over-sampling Technique) algorithm.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SMOTE creates new synthetic points in order to have an equal balance of the classes. This is another alternative for solving the \"class imbalance problems\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "sm = SMOTE(random_state=27)#, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling the under-represented class using SMOTE algorithm and LogisticRegression\n",
    "smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, smote_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, smote_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, smote_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, smote_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, smote_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)    \n",
    "\n",
    "# f1 score : 0.1235356762513312\n",
    "# precision : 0.06643757159221077\n",
    "# recall : 0.8787878787878788\n",
    "# Average Precision-Recall Score : 0.058609245400303336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling the under-represented class using SMOTE algorithm and RandomForestClassifier\n",
    "smote = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, smote_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, smote_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, smote_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, smote_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, smote_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision)  \n",
    "\n",
    "# f1 score : 0.865079365079365\n",
    "# precision : 0.9083333333333333\n",
    "# recall : 0.8257575757575758\n",
    "# Average Precision-Recall Score : 0.7503861559472708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using KNeighborsClassifier\n",
    "\n",
    "smote = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, smote_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, smote_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, smote_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, smote_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, smote_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score : 0.24137931034482757\n",
      "precision : 0.1407035175879397\n",
      "recall : 0.8484848484848485\n",
      "Accuracy : 0.9901126372854695\n",
      "Average Precision-Recall Score : 0.11966569378809648\n"
     ]
    }
   ],
   "source": [
    "# Modeling the upsampled data as is using SVC\n",
    "\n",
    "smote = svm.SVC().fit(X_train, y_train)\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, smote_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, smote_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, smote_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, smote_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, smote_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the upsampled data as is using MLPClassifier\n",
    "\n",
    "smote = MLPClassifier(activation = \"logistic\").fit(X_train, y_train)\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "f1 = f1_score(y_test, smote_pred)\n",
    "print(\"f1 score :\", f1)\n",
    "precision = precision_score(y_test, smote_pred)\n",
    "print(\"precision :\",precision)\n",
    "recall = recall_score(y_test, smote_pred)\n",
    "print(\"recall :\",recall)\n",
    "accuracy = accuracy_score(y_test, smote_pred)\n",
    "print(\"Accuracy :\",accuracy)\n",
    "average_precision = average_precision_score(y_test, smote_pred)\n",
    "print(\"Average Precision-Recall Score :\",average_precision) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
